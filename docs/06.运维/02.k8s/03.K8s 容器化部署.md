---
title: K8s 容器化部署
order: 2
group: 
  title: Kubernetes
date: 2021-02-07 12:36:30
permalink: /pages/dd2b39/
categories: 
  - 运维
  - k8s
tags: 
  - null
comment: true
---

# K8s 容器化部署

### 1. 初始化系统

##### 1.1 安装系统工具

```
echo "将最大文件打开数修改成100001, 关闭selinux"
echo "*                -       nofile    100001"  >> /etc/security/limits.conf
echo "*                -       nproc     100001"  >> /etc/security/limits.conf
sed -i "s/SELINUXTYPE=targeted/#SELINUXTYPE=targeted/" /etc/selinux/config
sed -i "s/SELINUX=enforcing/SELINUX=disabled/" /etc/selinux/config

echo "安装netstat,wget,curl,git,unzip等工具 "
yum install wget iptables-services telnet net-tools git curl unzip sysstat lsof ntpdate lrzsz vim  -y
```

##### 1.2 安装ntp并同步时间

```
echo "安装ntp,并设置同步时间"
yum install ntp -y
systemctl start ntpd
systemctl enable ntpd
timedatectl set-timezone Asia/Shanghai
timedatectl set-ntp yes
timedatectl
```

##### 1.3 关闭防火墙

```
echo "关闭firwalld，并备份iptables"
systemctl stop firewalld.service
systemctl disable firewalld.service
mv /etc/sysconfig/iptables  /etc/sysconfig/iptables.bak

systemctl disable iptables.service
echo "停止iptables"
systemctl stop iptables.service

echo "优化sshd登陆"
echo "UseDNS no" >> /etc/ssh/sshd_config
echo "UseDNS no" >> /etc/ssh/sshd_config
systemctl restart sshd
```

### 2. 启用IPVS和安装相关工具

##### 2.1 启用IPVS

> 参考IPVS https://www.cnblogs.com/hongdada/p/9758939.html

```
在所有的Kubernetes节点执行以下脚本（若内核大于4.19替换nf_conntrack_ipv4为nf_conntrack）:
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

##### 2.2 安装工具

```
#安装相关管理工具
yum install ipset ipvsadm -y
#查看超时设置
ipvsadm -l --timeout 
#查看链接
ipvsadm -ln
```

### 3. 系统内核调整

```
cat <<EOF > /etc/sysctl.d/k8s.conf
# https://github.com/moby/moby/issues/31208 
# ipvsadm -l --timout
# 修复ipvs模式下长连接timeout问题 小于900即可
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 10

#调整系统系统令牌数量 
net.ipv4.tcp_max_tw_buckets = 100000
#调整k8s要求的相关参数 

net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
vm.swappiness=0
EOF

#重载内核参数
sysctl --system
```

### 4. 安装docker

```
cd /etc/yum.repos.d/
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

#安装docker存储相关工具
yum install -y yum-utils device-mapper-persistent-data lvm2

#指定版本k8s要求是18.06
#yum list docker-ce --showduplicates -y
yum install docker-ce*18.06*  -y

# 配置 daemon.阿里云镜像加速和systemd进程管理
#参考: https://kubernetes.io/docs/setup/production-environment/container-runtimes/
mkdir -pv /etc/docker
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "registry-mirrors": ["https://gfexkhvj.mirror.aliyuncs.com"]
}
EOF

#设置开机启动并启动docker
systemctl daemon-reload
systemctl restart docker
systemctl enable docker && systemctl restart docker
docker ps -a
```

#### 安装 Compose

```
wget http://docker.fengdai.org/docker/docker-compose
mv docker-compose  /usr/local/bin/
chmod +x /usr/local/bin/docker-compose
chown root:root /usr/local/bin/docker-compose

docker-compose -v
```



### 5. 安装kubeadmin

```
cd /etc/yum.repos.d/
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

#yum search kubelet --showduplicates -y
#安装指定的1.14.3版本
yum install -y kubeadm-1.14* kubelet-1.14* kubectl-1.14*
systemctl enable kubelet
```

### 6. 安装haproxy(做tcp代理，公有云可用SLB替代)

```
yum install haproxy -y 
mv /etc/haproxy/haproxy.cfg  /etc/haproxy/haproxy.cfg.bak

echo """
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon
    stats socket /var/lib/haproxy/stats

defaults
    mode                    tcp
    log                     global
    option                  tcplog
    option                  dontlognull
    option                  redispatch
    retries                 3
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout check           10s
    maxconn                 3000

listen stats
    mode   http
    bind :10086
    stats   enable
    stats   uri     /admin?stats
    stats   auth    admin:admin
    stats   admin   if TRUE
    
frontend  k8s_https *:8443
    mode      tcp
    maxconn      2000
    default_backend     https_sri
    
backend https_sri
    balance      roundrobin
    server master1-api 10.0.0.4:6443  check inter 10000 fall 2 rise 2 weight 1
    server master2-api 10.0.0.5:6443  check inter 10000 fall 2 rise 2 weight 1
    server master3-api 10.0.0.6:6443  check inter 10000 fall 2 rise 2 weight 1
""" > /etc/haproxy/haproxy.cfg

#启动haproxy 监控8443端口 代理到6443端口
#10.0.0.4 10.0.0.5 10.0.0.6 改成自已集群IP
systemctl enable haproxy && systemctl start haproxy 
```

### 7. 初始化k8s集群

##### 7.1 配置yaml文件

```
#生成yaml配置
kubeadm config print init-defaults > kubeadmin-config.yaml

#配置yaml文件
apiVersion: kubeadm.k8s.io/v1beta1
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.0.0.4
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: uat-csp-k8s-master1
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta1
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: "10.0.0.4:8443"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.14.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
featureGates:
  SupportIPVSProxyMode: true
mode: ipvs
```

![image](https://note.youdao.com/yws/public/resource/e22eaee7bf8cf88c20921b900d67998f/C8C9D93D80674152AFE3C5FF2C5CB012?ynotemdtimestamp=1565078664687)

##### 7.2 初始化k8s

```
配置hosts
echo """
10.0.0.4 uat-csp-k8s-master1
10.0.0.5 uat-csp-k8s-master2
10.0.0.6 uat-csp-k8s-master3
""" >> /etc/hosts
#1.13版本无此参数--experimental-upload-certs
#执行日志存一份在kubeadmin-init.log
kubeadm init --config=kubeadmin-config.yaml --experimental-upload-certs | tee kubeadmin-init.log

#上面执行成功后
cat << EOF >> ~/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF
source ~/.bashrc

#查看状态，如果coreDNS没有running，是因为网络组件没有安装
kubectl get cs
kubectl get nodes
kubectl -n kube-system get pods
```

![image](https://note.youdao.com/yws/public/resource/e22eaee7bf8cf88c20921b900d67998f/B4C257C901894D7F9EDAB1B0365635BA?ynotemdtimestamp=1565078664687)

##### 7.3 安装网络组件flannel

```
#下载flannel yaml文件
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
#查看镜像和子网段
cat kube-flannel.yml | grep -E "image|10.244"

#如果网络无法下镜像可替换镜像地址,一般都通的
#sed -i 's#quay.io/coreos/flannel:v0.11.0-amd64#willdockerhub/flannel:v0.11.0-amd64#g' kube-flannel.yml

#安装flannel
kubectl apply -f kube-flannel.yml

#等待3-5分钟，执行下列命令查看状态
kubectl get cs
kubectl get nodes -o wide
kubectl -n kube-system get pods -o wide
```

##### 7.4 其他2个节点加到master集群

```
要求:  系统初始化 docker kubectl等已安装完成
#执行上图中显示的命令
kubeadm join 10.0.0.4:8443 --token abcdef.0123456789abcdef \
>     --discovery-token-ca-cert-hash sha256:378cb6ab8b82daa523486804eee5f8a79c69c1273c3e76f79ea54388632bf521 \
>     --experimental-control-plane --certificate-key bbff2ac18c8ecb0f7d6f537e48b2f2e01e9893166ba8eb86a8dbb09bd3efca09
```

![image](https://note.youdao.com/yws/public/resource/e22eaee7bf8cf88c20921b900d67998f/6DE2D2AE378C47B9B4B72C72E5D1350B?ynotemdtimestamp=1565078664687)

```
cat << EOF >> ~/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF
source ~/.bashrc

kubectl get cs
kubectl get nodes -o wide
kubectl -n kube-system get pods -o wide
```

### 8.helm安装

```
在master1上执行
wget http://docker.fengdai.org/k8s/helm 
mv helm /usr/local/bin/ && chmod +x /usr/local/bin/helm
执行：
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
EOF
helm init --service-account tiller -i harbor.trc.com/devops/kubernetes-helm/tiller:v2.14.0 --stable-repo-url http://mirror.azure.cn/kubernetes/charts/
kubectl get pods --namespace=kube-system -o wide 会看到一个tiller的pod
helm version 查看正常即可
```

### 9.安装nginx-ingress

```
helm install stable/nginx-ingress --name dew-nginx --namespace nginx-ingress --version=1.4.0     --set controller.kind=DaemonSet     --set controller.hostNetwork=true     --set controller.stats.enabled=true     --set controller.metrics.enabled=true     --set defaultBackend.image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/defaultbackend --set defaultBackend.image.tag=1.4

helm list //查看helm仓库列表

helm del --purge dew-nginx //删除

kubectl -n nginx-ingress get pods 查看nginx-ingress内的pod，都是running即为正常
注：镜像拉取不下来时：
kubectl -n nginx-ingress   set image  ds dew-nginx-nginx-ingress-controller nginx-ingress-controller=harbor.trc.com/devops/nginx-ingress-controller:0.24.1
```

### 10.客户端挂载存储

```
参考文档：GlusterFS  kubernetes配置
```

### 11.prometheus&grafana

```
注：拉取prometheus镜像失败时执行
kubectl set  image deploy dew-prometheus-operator-kube-state-metrics kube-state-metrics=registry.cn-hangzhou.aliyuncs.com/google_containers/kube-state-metrics:v1.5.0 -n devops

遇到节点prometheus pod都为running，但报错无节点可用时：
kubectl label no uat-csp-k8s-node1 group=devops 将node1打标签为devops
```

### 12.minio

```
cd /opt && wget  https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio && mkdir /rundata (创建存储目录)
启动：
nohup ./minio server /rundata &
```

### 13.dashboard

```
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kube-system
type: Opaque
EOF
helm install stable/kubernetes-dashboard --name dew-dashboard --namespace kube-system --version=1.4.0 \
    --set image.repository=registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64 \
    --set rbac.clusterAdminRole=true \
    --set serviceAccount.create=true \
    --set ingress.enabled=true \
    --set-string ingress.annotations."nginx\.ingress\.kubernetes\.io/backend-protocol"="HTTPS" \
    --set ingress.hosts={test-dashboard.fengdai.org} \
    --set ingress.tls[0].hosts={test-dashboard.fengdai.org},ingress.tls[0].secretName=kubernetes-dashboard-certs \
    --set nodeSelector.group=devops

kubectl -n kube-system get pods

# 获取Token
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep dew-dashboard-kubernetes-dashboard | awk '{print $1}')

# 添加域名到客户机hosts并访问 https://dashboard.dew.ms
```