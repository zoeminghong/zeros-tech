---
title: Spark_SQL
date: 2021-07-18 21:55:02
permalink: /pages/8f9254/
categories:
  - 大数据
  - spark
tags:
  - 
---
## Spark SQL

- Spark 处理结构化数据的库
- 企业中处理报表数据

在 Spark 中，也支持 Hive 中的自定义函数。自定义函数大致可以分为三种：

- UDF(User-Defined-Function)，即最基本的自定义函数，类似 to_char,to_date 等
- UDAF(User- Defined Aggregation Funcation)，用户自定义聚合函数，类似在 group by 之后使用的 sum,avg 等
- UDTF(User-Defined Table-Generating Functions),用户自定义生成函数，有点像 stream 里面的 flatMap

### UDF

**e.g.**

```java
package test;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.ArrayList;
import java.util.List;

public class test3 {
    public static void main(String[] args) {
        //创建spark的运行环境
        SparkConf sparkConf = new SparkConf();
        sparkConf.setMaster("local[2]");
        sparkConf.setAppName("test-udf");
        JavaSparkContext sc = new JavaSparkContext(sparkConf);
        SQLContext sqlContext = new SQLContext(sc);
        //注册自定义方法
        sqlContext.udf().register("isNull", (String field,String defaultValue)->field==null?defaultValue:field, DataTypes.StringType);
        //读取文件
        JavaRDD<String> lines = sc.textFile( "C:\\test-udf.txt" );
        JavaRDD<Row> rows = lines.map(line-> RowFactory.create(line.split("\\^")));

        List<StructField> structFields = new ArrayList<StructField>();
        structFields.add(DataTypes.createStructField( "a", DataTypes.StringType, true ));
        structFields.add(DataTypes.createStructField( "b", DataTypes.StringType, true ));
        structFields.add(DataTypes.createStructField( "c", DataTypes.StringType, true ));
        StructType structType = DataTypes.createStructType( structFields );

        DataFrame test = sqlContext.createDataFrame( rows, structType);
        test.registerTempTable("test");

        sqlContext.sql("SELECT con_join(c,b) FROM test GROUP BY a").show();
        sc.stop();
    }
}
```

### UDAF

Spark 为所有的 UDAF 定义了一个父类 UserDefinedAggregateFunction。要继承这个类，需要实现父类的几个抽象方法

```scala
// UDAF与DataFrame列有关的输入样式
def inputSchema: StructType

// 定义存储聚合运算时产生的中间数据结果的Schema
def bufferSchema: StructType

// UDAF函数的返回值类型
def dataType: DataType

// 用以标记针对给定的一组输入，UDAF是否总是生成相同的结果
def deterministic: Boolean

// 对聚合运算中间结果的初始化
def initialize(buffer: MutableAggregationBuffer): Unit

// 第一个参数为bufferSchema中两个Field的索引，默认以0开始
// 第二个参数input: Row对应的并非DataFrame的行，而是被inputSchema投影了的行。
def update(buffer: MutableAggregationBuffer, input: Row): Unit

// 负责合并两个聚合运算的buffer,再将其存储到MutableAggregationBuffer
def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit

// 完成对聚合Buffer值的运算
def evaluate(buffer: Row): Any

```

**e.g.**

```java
package test;

import org.apache.spark.sql.Row;
import org.apache.spark.sql.expressions.MutableAggregationBuffer;
import org.apache.spark.sql.expressions.UserDefinedAggregateFunction;
import org.apache.spark.sql.types.DataType;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.ArrayList;
import java.util.List;

public class MyAvg extends UserDefinedAggregateFunction {

    @Override
    public StructType inputSchema() {
        List<StructField> structFields = new ArrayList<>();
        structFields.add(DataTypes.createStructField( "field1", DataTypes.StringType, true ));
        return DataTypes.createStructType( structFields );
    }

    @Override
    public StructType bufferSchema() {
        List<StructField> structFields = new ArrayList<>();
        structFields.add(DataTypes.createStructField( "field1", DataTypes.IntegerType, true ));
        structFields.add(DataTypes.createStructField( "field2", DataTypes.IntegerType, true ));
        return DataTypes.createStructType( structFields );
    }

    @Override
    public DataType dataType() {
        return DataTypes.IntegerType;
    }

    @Override
    public boolean deterministic() {
        return false;
    }

    @Override
    public void initialize(MutableAggregationBuffer buffer) {
        buffer.update(0,0);
        buffer.update(1,0);
    }

    @Override
    public void update(MutableAggregationBuffer buffer, Row input) {
        buffer.update(0,buffer.getInt(0)+1);
        buffer.update(1,buffer.getInt(1)+Integer.valueOf(input.getString(0)));
    }

    @Override
    public void merge(MutableAggregationBuffer buffer1, Row buffer2) {
        buffer1.update(0,buffer1.getInt(0)+buffer2.getInt(0));
        buffer1.update(1,buffer1.getInt(1)+buffer2.getInt(1));
    }

    @Override
    public Object evaluate(Row buffer) {
        return buffer.getInt(1)/buffer.getInt(0);
    }
}
```

```java
package test;

import com.tgou.standford.misdw.udf.MyAvg;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.SQLContext;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructField;
import org.apache.spark.sql.types.StructType;

import java.util.ArrayList;
import java.util.List;

public class test4 {
    public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf();
        sparkConf.setMaster("local[2]");
        sparkConf.setAppName("test");
        JavaSparkContext sc = new JavaSparkContext(sparkConf);
        SQLContext sqlContext = new SQLContext(sc);

        sqlContext.udf().register("my_avg",new MyAvg());

        JavaRDD<String> lines = sc.textFile( "C:\\test4.txt" );
        JavaRDD<Row> rows = lines.map(line-> RowFactory.create(line.split("\\^")));

        List<StructField> structFields = new ArrayList<StructField>();
        structFields.add(DataTypes.createStructField( "a", DataTypes.StringType, true ));
        structFields.add(DataTypes.createStructField( "b", DataTypes.StringType, true ));
        StructType structType = DataTypes.createStructType( structFields );

        DataFrame test = sqlContext.createDataFrame( rows, structType);
        test.registerTempTable("test");

        sqlContext.sql("SELECT my_avg(b) FROM test GROUP BY a").show();

        sc.stop();
    }
}
```

文本内容

```txt
a^3
a^6
b^2
b^4
b^6
```

结果

```
4
4
```

### DataFrame

一种类表结构的数据结构。一般为 RDD、List、Seq 等列表形式数据结构转换得到。

```scala
 val buffer: ArrayBuffer[DopVisitInfoDto] = new ArrayBuffer()
 ...
 val dopVisitInfoTable: DataFrame = spark.createDataFrame(buffer)
 val dop_visit_info = dopVisitInfoTable
      .withColumn("LOGGING_TIME", ($"DOP_VISIT_LOGGINGTIME" / 1000).cast(TimestampType))
      .withColumn("year", year($"LOGGING_TIME"))
      .withColumn("month", month($"LOGGING_TIME"))
      .withColumn("day", dayofmonth($"LOGGING_TIME"))
      .withColumn("hour", hour($"LOGGING_TIME"))
```

**cast**：转换字段数据格式为指定类型

**withColumn**：基于现有的数据字段数据，进行一些处理，得到新的数据，将其作为新的字段存储一下来

```scala
// 数据持久化
dop_visit_info.persist()
// 创建tempView
dop_visit_info.createOrReplaceTempView("dop_visit_info")
```

**persist()**：将数据进行持久化

**createOrReplaceTempView()**：创建一种类似于 Table 的数据格式，支持 SQL 化查询

```scala
    spark.sql(
      """select year,month,day,DOP_VISIT_URL,DOP_VISIT_REFERURL,
        |count(ID) as DOP_PV_CNT
        |from dop_visit_info group by year,month,day,DOP_VISIT_URL,DOP_VISIT_REFERURL
      """.stripMargin).rdd.map(result => {
      val year = result.getInt(0)
      val month = result.getInt(1)
      val day = result.getInt(2)
      val DOP_VISIT_URL = result.getString(3)
      val dop_date = year*10000 + month*100 +day
      Referral(year, month, day,dop_date, DOP_VISIT_URL, result.getString(4),DOP_VISIT_URL+result.getString(4),result.getLong(5))
    }).foreachPartition(partition => {
      val updateSql =
        """INSERT INTO dop_day_page_path(
          |  year, month, day, dop_date,dop_visit_url,dop_visit_referurl,dop_md5_url,dop_pv_cnt,create_by,create_time,update_by)
          |VALUES (?, ?, ?, ?, ?, ?, md5(?),?,'',now(),'') ON DUPLICATE KEY UPDATE dop_pv_cnt = ? """.stripMargin
      JDBCTemplate.init(parameter.mysqlUrl, parameter.mysqlUser, parameter.mysqlPassword,flag="mysql")
      partition.foreach(record => {
          // JDBC持久化操作
        JDBCTemplate.pick("mysql").executeUpdate(updateSql, Seq( record.year, record.month, record.day,record.dopDate,
          record.visitUrl, record.referralUrl, record.md5Url,record.pvCnt, record.pvCnt))
      })
    })
```

`Spark SQL`化查询，使用`foreachPartition`提高数据处理性能。

### 支持的文件格式

![](https://ws2.sinaimg.cn/large/006tNbRwly1fx5f8zd508j30k609ntao.jpg)
