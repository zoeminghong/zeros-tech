---
title: Spark_Structured
date: 2021-07-18 21:55:02
permalink: /pages/c8e238/
categories: 
  - 大数据
  - spark
tags: 
  - null
comment: true
---
## 入门文章

[Spark 大数据之 DataFrame 和 Dataset](https://zhuanlan.zhihu.com/p/29830732)

## 语法说明

### Json 格式

#### get_json_object

使用类似于 `JsonPath` 方式获取对象内部的数据

```scala
select(get_json_object('{"k": "foo", "v": 1.0}','$.k')).as(k)
```

#### from_json

将 Json 字符串进行转为指定的格式数据

```scala
select a.k from  (
select from_json('{"k": "foo", "v": 1.0}','k STRING, v STRING',map("","")) as a
)
```

#### to_json

可以把所有字段转化为 json 字符串

```scala
select to_json(struct(*)) AS value
```

### Struct

#### 常见数据类型

- ByteType: 代表 1 字节有符号整数. 数值范围： -128 到 127.
- ShortType: 代表 2 字节有符号整数. 数值范围： -32768 到 32767.
- IntegerType: 代表 4 字节有符号整数. 数值范围： -2147483648 t 到 2147483647.
- LongType: 代表 8 字节有符号整数. 数值范围： -9223372036854775808 到 9223372036854775807.
- FloatType: 代表 4 字节单精度浮点数。
- DoubleType: 代表 8 字节双精度浮点数。
- DecimalType: 表示任意精度的有符号十进制数。内部使用 java.math.BigDecimal.A 实现。
- BigDecimal 由一个任意精度的整数非标度值和一个 32 位的整数组成。
- String 类型
- StringType: 表示字符串值。
- Binary 类型
- BinaryType: 代表字节序列值。
- Boolean 类型
- BooleanType: 代表布尔值。
- Datetime 类型
- TimestampType: 代表包含的年、月、日、时、分和秒的时间值
- DateType: 代表包含的年、月、日的日期值

#### ArrayType

ArrayType(elementType, containsNull): 代表包含一系列类型为 elementType 的元素。如果在一个将 ArrayType 值的元素可以为空值，containsNull 指示是否允许为空。

#### Map 格式

使用 `MapType` 构建 Map 数据格式

```
import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{MapType, StringType, StructField, StructType}

case class Document(
  id: String,
  text: String,
  metadata: scala.collection.Map[String, String] = Map()
)

object Document {
  def apply(row: Row): Document = {
    Document(row.getString(0), row.getString(1), row.getMap[String, String](2))
  }

  val DocumentDataType: StructType = StructType(Array(
    StructField("id",StringType,nullable = true),
    StructField("text",StringType,nullable = true),
    StructField("metadata",MapType(StringType,StringType,valueContainsNull = true),nullable = true)
  ))
}
```

#### StructType

StructType(fields): 代表带有一个 StructFields（列）描述结构数据。
StructField(name, dataType, nullable): 表示 StructType 中的一个字段。name 表示列名、dataType 表示数据类型、nullable 指示是否允许为空。

### withColumn

用于添加新的字段，第一个参数为新的字段名称，第二个参数为字段值的生成规则

```scala
df.withColumn("_tmp", split($"columnToSplit", "\\.")).select(
  $"_tmp".getItem(0).as("col1"),
  $"_tmp".getItem(1).as("col2"),
  $"_tmp".getItem(2).as("col3")
).drop("_tmp")
```

### col

获取 Column 数据信息，也可以使用 \$ 符合方式替换

```scala
df.withColumn("col1", split(col("text"), "\\.").getItem(0))
  .withColumn("col2", split(col("text"), "\\.").getItem(1))
  .withColumn("col3", split(col("text"), "\\.").getItem(2))
  .show(false)

df.withColumn("_tmp", split($"columnToSplit", "\\."))
  .withColumn("col1", $"_tmp".getItem(0))
  .withColumn("col2", $"_tmp".getItem(1))
  .withColumn("col3", $"_tmp".getItem(2))
  .drop("_tmp")
```

## Kafka

### Source

构造 DataFrame

```scala
val conf = spark
      .readStream
      .format("kafka")
      .option("subscribe", topics)
      .option("kafka.bootstrap.servers", kafkaServers)
      .option("startingOffsets", startingOffsets)
      .option("kafkaConsumer.pollTimeoutMs", pollTimeoutMs)
      .option("key.deserializer", "StringDeserializer")
      .option("value.deserializer", "StringDeserializer")
      .option("fetchOffset.numRetries", numRetries)
      .option("fetchOffset.retryIntervalMs", retryIntervalMs)
      .option("failOnDataLoss", failOnDataLoss)
    if (maxOffsetsPerTrigger != -1) {
      conf.option("maxOffsetsPerTrigger", maxOffsetsPerTrigger)
    }
    conf.load()
```
