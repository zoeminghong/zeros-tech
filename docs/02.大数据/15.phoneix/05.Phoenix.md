---
title: Phoenix
date: 2021-07-18 21:55:02
permalink: /pages/83c2a8/
categories: 
  - 大数据
  - phoneix
tags: 
  - null
comment: true
---
[TOC]
phoenix 更适合数据量的点查 或者 大数据量中过滤条件命中小数量的扫描查询 这里可以根据需求创建索引。查询模式相对固定

## 使用

`<>`标识为可更换
在 HBase 中创建的表不能再 Phoenix 显示

### JDBC 连接

```
jdbc:phoenix:10.200.138.234,10.200.138.235,10.200.138.236:2181:/hbase-unsecure

jdbc:phoenix:thin:url=http://10.211.6.55:8765;serialization=PROTOBUF
```

### 常用 QuerySerice 指令

```shell
# 进入PhoenixQuery服务环境
cd /opt/phoenix
bin/sqlline.py <10.200.131.25:2181>
# 退出
!quit

// 导入数据
bin/psql.py localhost data.csv
```

> 使用 psql 的时候，csv 文件中的值不能为 null，sql 文件中的语句需单条操作，不支持进行批量操作
> [参考文档](https://phoenix.apache.org/bulk_dataload.html)

关于 Pheonix 相关的命令开始都要加`!`符号

| 命令                  | 解释                 | 示例                 |
| --------------------- | -------------------- | -------------------- |
| !tables               | 显示所有表           | !table               |
| !describe [tableName] | 显示表结构           | !describe user       |
|                       | !columns [tableName] | 显示当前表所有字段名 |
|                       | !quit                | 退出 Phoenix 命令    |
|                       | !help                | 帮助指令             |

[全部指令](https://www.baidufe.com/item/be5d573acd57ffda649f.html)

### SQL

```sql
DROP TABLE IF EXISTS USER;

create table IF NOT EXISTS USER   ( id varchar not null primary key, trc_shop_order_user_sex integer);
```

> 在创建表/检索的时候，对字段使用`""`包裹，字段是区分大小写的，不加则统一转为大写

#### 支持的类型

| 类型          |     |
| ------------- | --- |
| char          |     |
| varchar       |     |
| decimal       |     |
| tinyint       |     |
| smallint      |     |
| integer       |     |
| bigint        |     |
| float         |     |
| double        |     |
| timestamp     |     |
| date          |     |
| time          |     |
| binary        |     |
| varbinary     |     |
| unsigned_long |     |

**注意**

- 数值类型时间不能使用 Date 类型
- date 类型是到年月日，timestamp 才是日期加时间

#### 常用语句

##### 创建表

```
CREATE TABLE EventLog (
    eventId BIGINT NOT NULL,
    eventTime TIME NOT NULL,
    cf.eventType CHAR(3)
    CONSTRAINT pk PRIMARY KEY (eventId, eventTime)) COLUMN_ENCODED_BYTES=0;
```

> 如果创建的表已经存在，且其不是通过 phoenix create 语法创建的，则可以继续使用 phoenix create 创建同名表，不影响原有表数据，但会影响通过 phoenix select 结果值存在部分不需要的值。

##### 修改表

```
alter table EventLog add ef.lastGCTime Time null;
```

##### 插入更新

```
UPSERT INTO TEST(NAME,ID) VALUES('foo',123)
```

upsert 除了正常的方式之外还支持，将另外一个查询的结果作为值插入表中。如果 auto commit 开启的话，会在服务端就提交了，否则会缓存到客户端，等着显式提交的时候进行批量 upsert，通过配置” phoenix.mutate.upsertBatchSize”指定大小，默认 10000 行/次。

phoenix 中的主键对应的就是 hbase 中的 rowkey

##### 索引

index：二级索引。在表或视图上创建二级索引，当前版本仅支持对具有 IMMUTABLE_ROWS 属性的表上添加二级索引。目前实现是在数据行插入后便插入了索引。当创建了索引后，其实也会在 HBase 中创建一张表，表名为该二级索引名，所以还可对该 index 指定创建表相关参数。同时还可删除索引和修改索引。

```
CREATE INDEX my_idx ON sales.opportunity(last_updated_date DESC)

DROP INDEX my_idx ON sales.opportunity

ALTER INDEX my_idx ON sales.opportunity DISABLE
```

##### explain

执行计划。提供一个很简单的方式查看执行给定命令所需的逻辑步骤。每个步骤局势以单行字符串进行输出表示。这个可以很容易定位查询的性能瓶颈，或者所建二级索引是否生效等。

```
explain select * from test where age>0;
```

##### SALT_BUCKETS

Phoenix Salted Table 是 phoenix**为了防止 hbase 表 rowkey 设计为自增序列而引发热点 region 读和热点 region**写而采取的一种表设计手段。通过在创建表的时候指定 SALT_BUCKETS 来实现 pre-split(预分割)。如下表示创建表的时候将表预分割到 20 个 region 里面。

###### 原理

Phoenix Salted Table 的实现原理是在将一个散列取余后的 byte 值插入到 rowkey 的第一个字节里，并通过定义每个 region 的 start key 和 end key 将数据分割到不同的 region，以此来防止自增序列引入的热点问题，从而达到平衡 HBase 集群的读写性能的目的。

salted byte 的计算方式大致如下：

```
hash(rowkey) % SALT_BUCKETS
```

> hash(rowkey) % SALT_BUCKETS

默认下 salted byte 将作为每个 region 的 start key 及 end key，以此分割数据到不同的 region，这样能做到具有相同 salted byte 的数据能够位于同一个 region 里面。

phoenix 在插入数据的时候会计算一个 byte 字段并将这个字节插入到 rowkey 的首位置上；而在读取数据的 API 里面也相应地进行了处理，跳过(skip)第一个字节从而读取到正确的 rowkey(注意只有 salted table 需要这么处理)，所以只能通过 phoenix 接口来获取数据已确保拿到正确的 rowkey。

##### 联表查询

在进行多表查询的时候，你可能会需要对表进行重命名操作，这个时候通过 AS 语句就可以实现

```
SELECT * FROM `t_prop_ident` pi LEFT JOIN `t_rel_party_prop` rpp ON  pi.id=rpp.`to_id`
```

##### 缓存

NO_Cache 可以实现查询不使用 Hbase 缓存效果

```
select /*+ NO_CACHE */ val from ptsdb
```

[Grammar](https://phoenix.apache.org/language/index.html)

[支持的函数](https://blog.csdn.net/huanggang028/article/details/12563481)

#### 动态创建字段

```
CREATE TABLE EventLog (
    eventId BIGINT NOT NULL,
    eventTime TIME NOT NULL,
    eventType CHAR(3)
    CONSTRAINT pk PRIMARY KEY (eventId, eventTime)) COLUMN_ENCODED_BYTES=0

SELECT eventId, eventTime, lastGCTime, usedMemory, maxMemory FROM EventLog(lastGCTime TIME, usedMemory BIGINT, maxMemory BIGINT) where eventId=1

UPSERT INTO EventLog (eventId, eventTime, eventType, lastGCTime TIME, usedMemory BIGINT, maxMemory BIGINT) VALUES(1, CURRENT_TIME(), 'abc', CURRENT_TIME(), 512, 1024);
```

[https://yq.aliyun.com/articles/576440](https://yq.aliyun.com/articles/576440)

### 修改表结构

[link](https://blog.csdn.net/yuanhaiwn/article/details/82141923)

## 事务

### 开启事务

修改 client 侧的 hbase-site.xml

```
<property>
  <name>phoenix.transactions.enabled</name>
  <value>true</value>
</property>
```

修改 server 侧 hbase-site.xml

```
<property>
  <name>data.tx.snapshot.dir</name>
  <value>/tmp/tephra/snapshots</value>
</property>

<property>
  <name>data.tx.timeout</name>
  <value>60</value>
</property>
```

配置 \$HBASE_HOME 和开启事务管理器

```
./bin/tephra
```

### 使用事务

```
CREATE TABLE my_table (k BIGINT PRIMARY KEY, v VARCHAR) TRANSACTIONAL=true;

ALTER TABLE my_other_table SET TRANSACTIONAL=true;
```

[事务参考](https://phoenix.apache.org/transactions.html)

## 问题与解决方案

### hbase 服务异常

> 在 ch'g

```
Error: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions:
Mon May 21 16:05:03 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=60101: Call to prod-dmp6.fengdai.org/10.210.6.56:16020 failed on local exception: org.apache.hadoop.hbase.ipc.CallTimeoutException: Call id=1507, waitTime=60000, operationTimeout=59999 expired. row '�&&�' on table 'FDN_UCENTER_1_1_0_T_REL_PARTY_PROP' at region=FDN_UCENTER_1_1_0_T_REL_PARTY_PROP,\x80\x0Db\x08,1526886240522.6af7f022696e191d13014c3258dd3b73., hostname=prod-dmp6.fengdai.org,16020,1526009568364, seqNum=82636 (state=08000,code=101)
```

#### 解决方法

> 重启 Hbase 服务

### sqlline 与程序执行 sql 结果不一致

> phoenix server 服务同步不一致

#### 解决方法

> 重启 Phoenix Server 服务

## 数据导入导出

### 导出

```
!outputformat csv
!record data.csv
select * from system.catalog limit 10;
!record
!quit
```

### 导入

[https://phoenix.apache.org/bulk_dataload.html](https://phoenix.apache.org/bulk_dataload.html)

## 索引

Select 查询中多个字段作为查询条件的场景？

一般存在两种实现方式。第一种是每个字段单独建索引，第二种是组合索引，而且要求跟查询第一个字段要一致。

在实践的过程中发现，第一种方案不适合多个字段联合查询的场景。

可得出：

- < > <= >=,is null 的查询条件也是支持索引命中的
- 为空，索引不命中
- 单字段作为查询条件。1、值为空的时候，查询效率很快。2、基于该字段创建两个字段的索引，索引命中。3、创建一个以该字段为开始的组合索引，索引命中
- 双字段作为查询条件。无关乎顺序，索引命中。
- 多个字段作为查询条件。创建索引要与查询条件一致，推荐能筛选数据返回越多的字段作为第一个索引字段。支持差字段（如：a,b,c 作为索引，a,b 作为查询条件）

```sql
CREATE LOCAL INDEX IF NOT EXISTS IDX_VISIT_INFO_loggingTime_userId ON DOP_VISIT_INFO(dop_visit_loggingTime,DOP_VISIT_USERID);

// 命中
SELECT * FROM dop_visit_info WHERE dop_visit_userId = 'cd6da706220c459f9089478e1bd0fc6f' and dop_visit_loggingTime =1541088000000;

CREATE LOCAL INDEX IF NOT EXISTS IDX_VISIT_INFO_loggingTime_userId ON DOP_VISIT_INFO(dop_visit_loggingTime,DOP_VISIT_USERID,DOP_VISIT_URL);

// 命中
SELECT * FROM dop_visit_info WHERE dop_visit_userId = 'cd6da706220c459f9089478e1bd0fc6f' and dop_visit_loggingTime =1541088000000;
```

### 全局索引

```
create index if not exists IDX_GLOBAL_FDN_UCENTER_1_1_0_V_OAUTH_AUTHORIZE_V2_USER ON fdn_ucenter_1_1_0_v_oauth_authorize_v2 (fdn_to_user_union_id,fdn_from_org_id,fdn_to_org_id,fdn_del_flag) include(fdn_from_user_union_id);
```

由 fdn_to_user_union_id,fdn_from_org_id,fdn_to_org_id,fdn_del_flag 合起来创建 RowKey，并有参数为 fdn_from_user_union_id

### 删除本地索引

1. 在 Phoenix 中删除索引

```
drop index IDX_VISIT_INFO_loggingTime_userId on DOP_VISIT_INFO;
```

2. 在 hbase shell 中删除相应表中的索引列族

```
hbase shell

# NAME 列族，METHOD 操作方法
# 表名区分大小写
alter 'dop_visit_info', {NAME => 'L#0', METHOD => 'delete'}
```

注意：删除列族会导致该表的所有索引都被删除，索引需要被重新创建

## 关联查询

两个结果集过大的表，做 inner join 操作。where 条件过滤后，结果集分别是 100W，50W。走二级索引，也快不起来的。速度是慢在关联上面。phoenix 表关联查询，要避免两个大结果集关联的情况。它更适合大表与小表的 join。比如官方的 5000 万、1000 条。或者通过条件制造，大、小表的 join 查询。

补充：两个结果集都很大的表，关联。还可以使用`/*+ USE_SORT_MERGE_JOIN */` 处理。
